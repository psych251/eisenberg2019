---
title: "Reproducibility Report for Uncovering the structure of self-regulation through data-driven ontology discover by Eisenberg et al. (2019, Nature Communication)"
author: "Mia Jimenez-Fuentes (miafuen@stanford.edu)"
date: "`r format(Sys.time(), '%B %d, %Y')`"
format:
  html:
    toc: true
    toc_depth: 3
---

<!-- Reproducibility reports should all use this template to standardize reporting across projects. These reports will be public supplementary materials that accompany the summary report(s) of the aggregate results. -->

## Introduction

I propose to reproduce the findings presented in *Uncovering the structure of self-regulation through data-driven ontology discover*, where Eisenberg et al. (2019) explore the idea of defining and measuring the concept of self-regulation. The key analyses of interest here are factor anlalyses which identify latent dimensions underlying nearly 200 task and survey variables, and the clustering analyses, which map how those measures group together. 

### Justification for choice of study

In *Uncovering the structure of self-regulation through data-driven ontology discover*, Eisenberg et al. (2019) provides a compelling example of how large-scale, complicated, data-driven methods can be used to clarify the meaning and measurement of psychological constructs that are otherwise incredibly difficult to pin down. In this paper, the authors explore the idea of self-regulation. This concept has been tied to various outcomes ranging from health to education, but the field has consistently debated how to measure it and, more broadly, if it is a coherent construct at all. This idea comes up frequently in discussions around constructs like these, especially so in conversations around skills of executive functions. This paper tackles this issue by integrating a broad battery of tasks, surveys, and real-world outcomes, offering a unique opportunity to examine how different measures related to one another. 

Reproducing the results of this study will be valuable for several reasons. To begin, the authors' methods are broad and complex - involving large-scale data collection, exploratory factor analysis, clustering, and predictive modeling. Reproducing their approach offers an opportunity to practice key skills in dimensional reduction, psychometrics, and prediction. Furthermore, this paper raises important conceptual issues about the reliability and validity of common measures in psychology - issues central to my own developing research interests. Overall, this work is methodologically complex and incredibly theoretically relevant. It will allow me to deepens my understanding of advanced data techniques while also informing my ongoing exploration about the measurement and meaning of related constructs in research. 

### Anticipated challenges

One challenge I anticipate surrounds the complexity of the analytic pipeline - the authors rely on several advanced methods and small deviations in parameter choices or implementation could lead to different results. Secondly, the dataset includes nearly 200 independent variables derived from the survey and behavioral tasks and reproducing the results will require careful preprocessing to match the authors' original approach.  

### Links

Project repository: https://github.com/psych251/eisenberg2019

Original paper: https://github.com/psych251/eisenberg2019/blob/main/original_paper/eisenberg2019pdf.pdf

## Methods

### Description of the steps required to reproduce the results

Please describe all the steps necessary to reproduce the key result(s) of this study. 

### Differences from original study

Explicitly describe known differences in the analysis pipeline between the original paper and yours (e.g., computing environment). The goal, of course, is to minimize those differences, but differences may occur. Also, note whether such differences are anticipated to influence your ability to reproduce the original results.

## Project Progress Check 1

### Measure of success

Please describe the outcome measure for the success or failure of your reproduction and how this outcome will be computed.


### Pipeline progress

Earlier in this report, you described the steps necessary to reproduce the key result(s) of this study. Please describe your progress on each of these steps (e.g., data preprocessing, model fitting, model evaluation).


## Results

### Data preparation

Data preparation following the analysis plan.
	
```{r include=F}
### Data Preparation

#### Load Relevant Libraries and Functions

#### Import data

#### Data exclusion / filtering

#### Prepare data for analysis - create columns etc.
```

### Key analysis

The analyses as specified in the analysis plan.  

*Side-by-side graph with original graph is ideal here*

### Exploratory analyses

Any follow-up analyses desired (not required).  

## Discussion

### Summary of Reproduction Attempt

Open the discussion section with a paragraph summarizing the primary result from the key analysis and assess whether you successfully reproduced it, partially reproduced it, or failed to reproduce it.  

### Commentary

Add open-ended commentary (if any) reflecting (a) insights from follow-up exploratory analysis of the dataset, (b) assessment of the meaning of the successful or unsuccessful reproducibility attempt - e.g., for a failure to reproduce the original findings, are the differences between original and present analyses ones that definitely, plausibly, or are unlikely to have been moderators of the result, and (c) discussion of any objections or challenges raised by the current and original authors about the reproducibility attempt (if you contacted them).  None of these need to be long.
