---
title: "Reproducibility Report for Uncovering the structure of self-regulation through data-driven ontology discover by Eisenberg et al. (2019, Nature Communication)"
author: "Mia Jimenez-Fuentes (miafuen@stanford.edu)"
date: "`r format(Sys.time(), '%B %d, %Y')`"
format:
  html:
    toc: true
    toc_depth: 3
---

<!-- Reproducibility reports should all use this template to standardize reporting across projects. These reports will be public supplementary materials that accompany the summary report(s) of the aggregate results. -->

## Introduction

I propose to reproduce the findings presented in *Uncovering the structure of self-regulation through data-driven ontology discover*, where Eisenberg et al. (2019) explore the idea of defining and measuring the concept of self-regulation. The key analyses of interest here are factor anlalyses which identify latent dimensions underlying nearly 200 task and survey variables, and the clustering analyses, which map how those measures group together. 

### Justification for choice of study

In *Uncovering the structure of self-regulation through data-driven ontology discover*, Eisenberg et al. (2019) provides a compelling example of how large-scale, complicated, data-driven methods can be used to clarify the meaning and measurement of psychological constructs that are otherwise incredibly difficult to pin down. In this paper, the authors explore the idea of self-regulation. This concept has been tied to various outcomes ranging from health to education, but the field has consistently debated how to measure it and, more broadly, if it is a coherent construct at all. This idea comes up frequently in discussions around constructs like these, especially so in conversations around skills of executive functions. This paper tackles this issue by integrating a broad battery of tasks, surveys, and real-world outcomes, offering a unique opportunity to examine how different measures related to one another. 

Reproducing the results of this study will be valuable for several reasons. To begin, the authors' methods are broad and complex - involving large-scale data collection, exploratory factor analysis, clustering, and predictive modeling. Reproducing their approach offers an opportunity to practice key skills in dimensional reduction, psychometrics, and prediction. Furthermore, this paper raises important conceptual issues about the reliability and validity of common measures in psychology - issues central to my own developing research interests. Overall, this work is methodologically complex and incredibly theoretically relevant. It will allow me to deepens my understanding of advanced data techniques while also informing my ongoing exploration about the measurement and meaning of related constructs in research. 

### Anticipated challenges

One challenge I anticipate surrounds the complexity of the analytic pipeline - the authors rely on several advanced methods and small deviations in parameter choices or implementation could lead to different results. Secondly, the dataset includes nearly 200 independent variables derived from the survey and behavioral tasks and reproducing the results will require careful preprocessing to match the authors' original approach.  

### Links

Project repository: https://github.com/psych251/eisenberg2019

Original paper: https://github.com/psych251/eisenberg2019/blob/main/original_paper/eisenberg2019pdf.pdf

## Methods

### Description of the steps required to reproduce the results

Steps necessary for reproducing Eisenberg 2019:

* Data Access and Setup: 
  * Identify data used in the original study (processed file is meaningful_variables_imputed.csv)
  * Confirm variable count, number of participants, and match to original paper more broadly 
  * Identify meaningful scripts for reference (buried within different analysis folders, primarly within dimensional_structure/ and graph_anlaysis/)
* Reproduce Figure 2 (Factor/ Dimensional Structure)
  * Compute inter-variable correlation matrix 
  * Run exploratory factor analysis (EFA) using extraction and rotation methods 
  * Determine dimensinoality using parallel analysis 
* Reproduce Figure 3 (Ontological Graph)
  * Construct network representation using the correlation matrix 
  * Apply graph-based clustering/ hierarchical grouping 
  * Visualize ontology structure 

### Differences from original study

* Computing environment 
  * The original paper uses a mix of Python and R 
* Data preprocessing 
  * Original imputation not reproduced, instead starting from author's finalized imputed dataset 

## Project Progress Check 1

### Measure of success

The primary success criterion is whether I can produce the two figures -- specifically the sparse partial-correlation network with a clearly separated task vs survey DV structure and the subsequent factor/cluster based ontology which involves extracting latent dimensions of hierarchical communities from the space constructed in figure 2. Both outcomes will be evaluated by structure/ visual alignment with the original paper and quantitatively (by checking counts of things like dependent variables (DVs) and subjects). 

### Pipeline progress

* Data Access and Setup: 
  * Identified and loaded the authors' finalized imputed datasets
  * Confirmed participant count and DV structure are consistent with the original study 
* Reproduce Figure 2 (Factor/ Dimensional Structure)
  * Successfully computed the partial correlation network 
    * DV counts match the paper's, but the breakdown between task and survey are off by 5. Will have to revisit to understand how to adjust the split
* Reproduce Figure 3 (Ontological Graph)
  * Have not yet started; will have to finish the dimensionality estimation step that is part of Figure 2

## Results

### Data preparation

Data preparation following the analysis plan.
	
```{r include=F}
### Data Preparation
# setting static variables 
# edge_metric <- "glasso" 
threshold_abs <- 0.05
set.seed(2025)

#### Load Relevant Libraries and Functions
library(tidyverse)
library(here)
library(qgraph)
library(psych)
library(ggplot2)

#### Import data
data_file <- here("data/meaningful_variables_imputed.csv") 
out_dir <- here("results", "figure2_glasso") # where to save image to
if (!dir.exists(out_dir)) dir.create(out_dir, recursive = TRUE) # creates folder if not already there 

df <- read.csv(data_file, stringsAsFactors = FALSE, check.names = TRUE)

#### Data exclusion / filtering


#### Prepare data for analysis - create columns etc.
if ("X" %in% names(df)) { # if there is an ID column called "X" (where subjectId is saved)
  df <- df[!is.na(df$X) & df$X != "", ] # drop rows with missing IDs
  rownames(df) <- df$X # set IDs as rownames
  df$X <- NULL # remove ID column from data
}

num_cols <- vapply(df, is.numeric, logical(1)) # identify numeric variables
df <- df[, num_cols, drop = FALSE] # keep numeric columns only
# matches paper vv 
# cat("→ N participants:", nrow(df), " | N DVs:", ncol(df), "\n") # checking sample size and number of DVs 


vars <- colnames(df) # defining variable names
is_survey <- grepl("survey", vars, ignore.case = TRUE) # finding the survyes by which ones have the word survey in them
# grouping task and survey for further analysis 
groups <- list(
  Task   = vars[!is_survey],
  Survey = vars[is_survey]
)

vars <- colnames(df) # defining variable names
is_survey <- grepl("survey", vars, ignore.case = TRUE) # finding the survyes by which ones have the word survey in them
groups <- list(Task = vars[!is_survey], Survey = vars[is_survey]) # grouping task and survey for further analysis 
n_task <- sum(!is_survey); n_survey <- sum(is_survey) # counting the tasks 

# also matches paper vv 
# cat("→ Tasks:", n_task, "  Surveys:", n_survey, "\n") # checking task split 
```

### Key analysis

The analyses as specified in the analysis plan.  

*Side-by-side graph with original graph is ideal here*

```{r fig-2-reproduction, echo=F}
# connectivity matrix 
cor_mat <- cor(df, use = "pairwise.complete.obs", method = "pearson") # base correlation
n_subj  <- nrow(df)
adj_mat <- EBICglasso(S = cor_mat, n = n_subj) # partial correlation adjacency matrix

# threshold edge list 
Adj_thr <- adj_mat
Adj_thr[abs(Adj_thr) < threshold_abs] <- 0
# cat("→ Retained edges ≥", threshold_abs, ":", 
#     sum(Adj_thr[upper.tri(Adj_thr)] != 0), "\n")

# layout from thresholded graph
lay <- qgraph(Adj_thr, layout = "spring", DoNotPlot = TRUE)$layout

# colors and graph visual parameters 
node_colors <- ifelse(is_survey, "red", "blue")
edge_col <- adjustcolor("gray", alpha.f = 0.5)
vsize_auto <- 2.4

# clean plot (not labeled)
# png(file.path(out_dir, "network_glasso_abs_ge_0.05_clean.png"),
#     width = 1800, height = 1300, res = 170)
qgraph(
  Adj_thr,
  layout = lay,
  color = node_colors,
  groups = groups,
  legend = FALSE,
  labels = FALSE,
  vsize = vsize_auto,
  edge.color = edge_col, posCol = edge_col, negCol = edge_col,
  border.width = 1,
  title = "EBICglasso Network (|partial r| ≥ 0.05)"
)

# annotation
text(x = max(lay[,1]) * 1.15, 
     y = mean(range(lay[,2])) + 0.05,
     labels = paste0(n_task, " Task DVs"), col = "blue", cex = 1.2, adj = 0)
text(x = max(lay[,1]) * 1.15,
     y = mean(range(lay[,2])) - 0.05,
     labels = paste0(n_survey, " Survey DVs"), col = "red", cex = 1.2, adj = 0)

# dev.off()
```


### Exploratory analyses

Any follow-up analyses desired (not required).  

## Discussion

### Summary of Reproduction Attempt

Open the discussion section with a paragraph summarizing the primary result from the key analysis and assess whether you successfully reproduced it, partially reproduced it, or failed to reproduce it.  

### Commentary

Add open-ended commentary (if any) reflecting (a) insights from follow-up exploratory analysis of the dataset, (b) assessment of the meaning of the successful or unsuccessful reproducibility attempt - e.g., for a failure to reproduce the original findings, are the differences between original and present analyses ones that definitely, plausibly, or are unlikely to have been moderators of the result, and (c) discussion of any objections or challenges raised by the current and original authors about the reproducibility attempt (if you contacted them).  None of these need to be long.
